{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
    "# <font color='blue'>Data Science Academy</font>\n",
    "## <font color='blue'>IA Generativa e LLMs Para Processamento de Linguagem Natural</font>\n",
    "## <font color='blue'>Projeto 8 - Deploy</font>\n",
    "## <font color='blue'>Ajuste Fino com Nossos Próprios Dados e Deploy de LLM via App Web</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando e Carregando os Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# !pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_CPP_MIN_LOG_LEVEL=3\n"
     ]
    }
   ],
   "source": [
    "%env TF_CPP_MIN_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.gradio.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/gradio/\n",
    "!pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o LLM Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo que nós treinamos\n",
    "modelo_llm_dsa = AutoModelForCausalLM.from_pretrained(\"llm/llm_dsa_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(13, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-1): 2 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=13, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_llm_dsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
    "## Carregando o Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma classe chamada DSATokenizer, que é usada para tokenizar os números\n",
    "class DSATokenizer:\n",
    "    \n",
    "    # Método construtor da classe, que é executado quando um objeto dessa classe é criado\n",
    "    def __init__(self, numbers_qty = 10):\n",
    "        \n",
    "        # Lista de tokens possíveis que o tokenizador pode encontrar\n",
    "        vocab = ['+', '=', '-1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "        \n",
    "        # Definindo a quantidade de números que o tokenizador pode lidar\n",
    "        self.numbers_qty = numbers_qty\n",
    "        \n",
    "        # Definindo o token de preenchimento (padding)\n",
    "        self.pad_token = '-1'\n",
    "        \n",
    "        # Criando um dicionário que mapeia cada token para um índice único\n",
    "        self.encoder = {str(v):i for i,v in enumerate(vocab)}\n",
    "        \n",
    "        # Criando um dicionário que mapeia cada índice único de volta ao token correspondente\n",
    "        self.decoder = {i:str(v) for i,v in enumerate(vocab)}\n",
    "        \n",
    "        # Obtendo o índice do token de preenchimento no encoder\n",
    "        self.pad_token_id = self.encoder[self.pad_token]\n",
    "\n",
    "    # Método para decodificar uma lista de IDs de token de volta para uma string\n",
    "    def decode(self, token_ids):\n",
    "        return ' '.join(self.decoder[t] for t in token_ids)\n",
    "\n",
    "    # Método que é chamado quando o objeto da classe é invocado como uma função\n",
    "    def __call__(self, text):\n",
    "        # Dividindo o texto em tokens individuais e retornando uma lista dos IDs correspondentes\n",
    "        return [self.encoder[t] for t in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o objeto\n",
    "tokenizer = DSATokenizer(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+': 0,\n",
       " '=': 1,\n",
       " '-1': 2,\n",
       " '0': 3,\n",
       " '1': 4,\n",
       " '2': 5,\n",
       " '3': 6,\n",
       " '4': 7,\n",
       " '5': 8,\n",
       " '6': 9,\n",
       " '7': 10,\n",
       " '8': 11,\n",
       " '9': 12}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '+',\n",
       " 1: '=',\n",
       " 2: '-1',\n",
       " 3: '0',\n",
       " 4: '1',\n",
       " 5: '2',\n",
       " 6: '3',\n",
       " 7: '4',\n",
       " 8: '5',\n",
       " 9: '6',\n",
       " 10: '7',\n",
       " 11: '8',\n",
       " 12: '9'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando Função Para Previsões com o LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a função gera_solution com três parâmetros: input, solution_length e model\n",
    "def dsa_faz_previsao(entrada, solution_length = 6, model = modelo_llm_dsa):\n",
    "\n",
    "    # Colocando o modelo em modo de avaliação. \n",
    "    model.eval()\n",
    "\n",
    "    # Convertendo a entrada (string) em tensor utilizando o tokenizer. \n",
    "    # O tensor é uma estrutura de dados que o modelo de aprendizado de máquina pode processar.\n",
    "    entrada = torch.tensor(tokenizer(entrada))\n",
    "\n",
    "    # Iniciando uma lista vazia para armazenar a solução\n",
    "    solution = []\n",
    "\n",
    "    # Loop que gera a solução de comprimento solution_length\n",
    "    for i in range(solution_length):\n",
    "\n",
    "        # Alimentando a entrada atual ao modelo e obtendo a saída\n",
    "        saida = model(entrada)\n",
    "\n",
    "        # Pegando o índice do maior valor no último conjunto de logits (log-odds) da saída, \n",
    "        # que é a previsão do modelo para o próximo token\n",
    "        predicted = saida.logits[-1].argmax()\n",
    "\n",
    "        # Concatenando a previsão atual com a entrada atual. \n",
    "        # Isso servirá como a nova entrada para a próxima iteração.\n",
    "        entrada = torch.cat((entrada, predicted.unsqueeze(0)), dim = 0)\n",
    "\n",
    "        # Adicionando a previsão atual à lista de soluções e convertendo o tensor em um número Python padrão\n",
    "        solution.append(predicted.cpu().item())\n",
    "\n",
    "    # Decodificando a lista de soluções para obter a string de saída e retornando-a\n",
    "    return tokenizer.decode(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando o LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testa a função (ATENÇÃO: não foi assim que o modelo aprendeu)\n",
    "dsa_faz_previsao('3 + 5 ', solution_length = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 8'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testa a função (ATENÇÃO: foi assim que o modelo aprendeu)\n",
    "dsa_faz_previsao('3 + 5 =', solution_length = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 9'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testa a função (ATENÇÃO: foi assim que o modelo aprendeu)\n",
    "dsa_faz_previsao('8 + 1 =', solution_length = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
    "## Web App Para Deploy do LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para retornar a função que faz a previsão\n",
    "def dsa_retorna_previsao(entrada):\n",
    "    return dsa_faz_previsao(entrada, solution_length = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a web app\n",
    "webapp = gr.Interface(fn = dsa_retorna_previsao, \n",
    "                      inputs = [gr.Textbox(label = \"Dados de Entrada\", \n",
    "                                           lines = 1, \n",
    "                                           info = \"Os dados devem estar na forma: '1 + 2 =' com um único espaço entre cada caractere e apenas números de um dígito são permitidos.\")],\n",
    "                      outputs = [gr.Textbox(label = \"Resultado (Previsão do Modelo)\", lines = 1)],\n",
    "                      title = \"DSA Projeto 8 - Deploy do LLM\",\n",
    "                      description = \"Digite os dados de entrada e clique no botão Submit para o modelo fazer a previsão.\",\n",
    "                      examples = [\"5 + 3 =\", \"2 + 9 =\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://e6d8f412f35cfee532.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e6d8f412f35cfee532.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webapp.launch(share = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%watermark -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
