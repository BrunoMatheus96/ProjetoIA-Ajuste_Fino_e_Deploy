{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
    "# <font color='blue'>Data Science Academy</font>\n",
    "## <font color='blue'>IA Generativa e LLMs Para Processamento de Linguagem Natural</font>\n",
    "## <font color='blue'>Projeto 8 - Treinamento</font>\n",
    "## <font color='blue'>Ajuste Fino com Nossos Próprios Dados e Deploy de LLM via App Web</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando e Carregando os Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# !pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark.\n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_CPP_MIN_LOG_LEVEL=3\n"
     ]
    }
   ],
   "source": [
    "%env TF_CPP_MIN_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import accelerate\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, GPT2Tokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o LLM\n",
    "\n",
    "https://huggingface.co/gpt2\n",
    "\n",
    "O modelo terá a mesma arquitetura do GPT-2, mas com algumas modificações para torná-lo menor. \n",
    "\n",
    "As principais mudanças são: \n",
    "\n",
    "- O tamanho do vocabulário que será 13 (porque só vai lidar com números), mais o padding token, o \"+\" e o \"=\". \n",
    "\n",
    "- A janela de contexto suportará apenas 6 tokens, pois estamos interessados apenas em realizar a adição de dois dígitos únicos (de fato, prever o dígito de saída).\n",
    "\n",
    "O objetivo aqui é demonstrar como realmente funciona o processo de preparação dos dados para um LLM.\n",
    "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
    "Nota: LLMs não fazem Matemática (pelo menos não ainda). LLMs fazem a previsão da próxima palavra ou caracter. Fazer com que os LLMs sejam capazes de realizar operações matemáticas (bem como outras tarefas cognitivas) é uma área ativa de pesquisa chamada Reasoning. \n",
    "\n",
    "Colocamos uma descrição completa do conceito de Reasoning e a indicação de um paper de pesquisa no e-book disponível no Capítulo 17 do curso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho do vocabulário\n",
    "vocab_size = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprimento da sequência\n",
    "sequence_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprimento do resultado\n",
    "result_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprimento do contexto\n",
    "context_length = sequence_length + result_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros de configuração do modelo GPT-2\n",
    "config = AutoConfig.from_pretrained(\"gpt2\", \n",
    "                                    vocab_size = vocab_size, \n",
    "                                    n_ctx = context_length, \n",
    "                                    n_head = 4, \n",
    "                                    n_layer = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo\n",
    "modelo = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular o tamanho do modelo\n",
    "def dsa_calcula_tamanho_modelo(model):\n",
    "    return sum(t.numel() for t in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Modelo: 15.0M parâmetros\n"
     ]
    }
   ],
   "source": [
    "print(f'Tamanho do Modelo: {dsa_calcula_tamanho_modelo(modelo) / 1000 ** 2:.1f}M parâmetros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo tem 15 milhões de parâmetros em vez dos 111 milhões de parâmetros da configuração padrão \"gpt2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando Tokenizador Personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma classe chamada DSATokenizer, que é usada para tokenizar os números\n",
    "class DSATokenizer:\n",
    "    \n",
    "    # Método construtor da classe, que é executado quando um objeto dessa classe é criado\n",
    "    def __init__(self, numbers_qty = 10):\n",
    "        \n",
    "        # Lista de tokens possíveis que o tokenizador pode encontrar\n",
    "        vocab = ['+', '=', '-1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "        \n",
    "        # Definindo a quantidade de números que o tokenizador pode lidar\n",
    "        self.numbers_qty = numbers_qty\n",
    "        \n",
    "        # Definindo o token de preenchimento (padding)\n",
    "        self.pad_token = '-1'\n",
    "        \n",
    "        # Criando um dicionário que mapeia cada token para um índice único\n",
    "        self.encoder = {str(v):i for i,v in enumerate(vocab)}\n",
    "        \n",
    "        # Criando um dicionário que mapeia cada índice único de volta ao token correspondente\n",
    "        self.decoder = {i:str(v) for i,v in enumerate(vocab)}\n",
    "        \n",
    "        # Obtendo o índice do token de preenchimento no encoder\n",
    "        self.pad_token_id = self.encoder[self.pad_token]\n",
    "\n",
    "    # Método para decodificar uma lista de IDs de token de volta para uma string\n",
    "    def decode(self, token_ids):\n",
    "        return ' '.join(self.decoder[t] for t in token_ids)\n",
    "\n",
    "    # Método que é chamado quando o objeto da classe é invocado como uma função\n",
    "    def __call__(self, text):\n",
    "        \n",
    "        # Dividindo o texto em tokens individuais e retornando uma lista dos IDs correspondentes\n",
    "        return [self.encoder[t] for t in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vamos testar o tokenizador!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o objeto do tokenizador\n",
    "tokenizer = DSATokenizer(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '+',\n",
       " 1: '=',\n",
       " 2: '-1',\n",
       " 3: '0',\n",
       " 4: '1',\n",
       " 5: '2',\n",
       " 6: '3',\n",
       " 7: '4',\n",
       " 8: '5',\n",
       " 9: '6',\n",
       " 10: '7',\n",
       " 11: '8',\n",
       " 12: '9'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoder do tokenizador\n",
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos que o modelo aprenda a gerar o número 2 toda vez que ele receber como entrada: 1 + 1 =\n",
    "\n",
    "E para isso precisamos aplicar a tokenização com base no vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 0, 4, 1, 5]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testando o tokenizador\n",
    "tokenizer(\"1 + 1 = 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'14'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Testando o tokenizador (por que o erro?)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m9 + 5 = 14\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m, in \u001b[0;36mDSATokenizer.__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m     31\u001b[0m     \n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Dividindo o texto em tokens individuais e retornando uma lista dos IDs correspondentes\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit()]\n",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m     31\u001b[0m     \n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Dividindo o texto em tokens individuais e retornando uma lista dos IDs correspondentes\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit()]\n",
      "\u001b[0;31mKeyError\u001b[0m: '14'"
     ]
    }
   ],
   "source": [
    "# Testando o tokenizador (por que o erro?)\n",
    "tokenizer(\"9 + 5 = 14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 0, 8, 1, 4, 7]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testando o tokenizador\n",
    "tokenizer(\"9 + 5 = 1 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'19'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Testando o tokenizador\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m19 + 5 = 2 4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m, in \u001b[0;36mDSATokenizer.__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m     31\u001b[0m     \n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Dividindo o texto em tokens individuais e retornando uma lista dos IDs correspondentes\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit()]\n",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m     31\u001b[0m     \n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Dividindo o texto em tokens individuais e retornando uma lista dos IDs correspondentes\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit()]\n",
      "\u001b[0;31mKeyError\u001b[0m: '19'"
     ]
    }
   ],
   "source": [
    "# Testando o tokenizador (por que o erro?)\n",
    "tokenizer(\"19 + 5 = 2 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizando o Formato dos Dados\n",
    "\n",
    "O conjunto de dados deve ser criado neste formato:\n",
    "\n",
    "- Entrada: \"2 + 3 = 0\" onde os 4 primeiros caracteres representam a sequência de entrada e o quinto caractere representa o primeiro caracter da saída.\n",
    "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
    "- Saída: \"+ 3 = 0 5\" onde os 2 últimos dígitos representam o resultado da adição e os 3 primeiros dígitos são ignorados durante o treinamento e preenchidos com o pad.\n",
    "\n",
    "O resultado é um conjunto de dados de sequências tokenizadas de números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma classe chamada CriaDataset, que herda da classe Dataset do PyTorch\n",
    "class DSAOrganizaDataset(Dataset):\n",
    "\n",
    "    # Método construtor da classe, que é executado quando um objeto dessa classe é criado\n",
    "    def __init__(self, split, length = 6):\n",
    "        \n",
    "        # Verificando se a divisão do dataset (split) é 'treino' ou 'teste'\n",
    "        assert split in {'treino', 'teste'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "    \n",
    "    # Definindo o método len que retorna o tamanho do dataset. \n",
    "    # Nesse caso, o tamanho é fixo e igual a 1 milhão.\n",
    "    def __len__(self):\n",
    "        return 1000000 \n",
    "\n",
    "    # Definindo o método getitem que é usado para obter um item específico do dataset\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Criando uma lista com todos os números disponíveis que não são tokens de padding e são numéricos\n",
    "        available_numbers = [int(n) for n in tokenizer.decoder.values() if n != tokenizer.pad_token and str(n).isnumeric()]\n",
    "        \n",
    "        # Selecionando aleatoriamente números da lista de números disponíveis para criar uma entrada (input)\n",
    "        inp = torch.tensor(np.random.choice(available_numbers, size = result_length))\n",
    "        \n",
    "        # Calculando a soma dos números selecionados e criando um tensor\n",
    "        sol = torch.tensor([int(i) for i in str(inp.sum().item())])\n",
    "        \n",
    "        # Preenchendo o tensor com zeros para que tenha o tamanho desejado\n",
    "        sol = torch.nn.functional.pad(sol, (1 if sol.size()[0] == 1 else 0,0), 'constant', 0)\n",
    "\n",
    "        # Concatenando a entrada e a solução em um tensor\n",
    "        cat = torch.cat((inp, sol), dim = 0)\n",
    "\n",
    "        # Criando os tensores de entrada e alvo para o treinamento do modelo\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "\n",
    "        # Definindo o primeiro elemento do tensor alvo como o token de padding\n",
    "        y[:1] = int(tokenizer.pad_token)\n",
    "\n",
    "        # Transformando os tensores x e y em strings\n",
    "        x = str(x[0].item()) + ' + ' + str(x[1].item()) + ' = ' + str(x[2].item())\n",
    "        y = '-1 ' + str(y[0].item()) + ' -1 ' + str(y[1].item()) + ' ' + str(y[2].item())\n",
    "        \n",
    "        # Tokenizando as strings de entrada e alvo\n",
    "        tokenized_input = tokenizer(x)\n",
    "        tokenized_output = tokenizer(y)\n",
    "        \n",
    "        # Retornando os tensores de entrada e alvo como itens do dataset\n",
    "        return torch.tensor(tokenized_input), torch.tensor(tokenized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os Datasets de Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara o dataset de treino\n",
    "dataset_treino = DSAOrganizaDataset('treino', length = sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara o dataset de teste\n",
    "dataset_teste = DSAOrganizaDataset('teste', length = sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset_treino[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12,  0,  9,  1,  4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 4, 8])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 + 6 = 1\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(x.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 -1 -1 1 5\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(y.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o Loop de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "num_epochs = 2\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otimizador\n",
    "optimizer = torch.optim.Adam(modelo.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dados = torch.utils.data.DataLoader(dataset_treino, shuffle = True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/accelerate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o acelerador\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otimiza modelo, otimizador e dados para o dispositivo\n",
    "modelo, optimizer, dados = accelerator.prepare(modelo, optimizer, dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(13, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-1): 2 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=13, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arquitetura do modelo que será treinado\n",
    "modelo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o LLM (Ajuste Fino) com Nossos Próprios Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando o Ajuste Fino do LLM... Seja Paciente e Aguarde!\n",
      "\n",
      "Epoch: 1 / 2 --- Erro: 0.13560616970062256\n",
      "\n",
      "Epoch: 2 / 2 --- Erro: 0.023130610585212708\n",
      "\n",
      "Ajuste Fino do LLM Concluído com Sucesso.\n",
      "\n",
      "CPU times: user 4min 53s, sys: 44.6 s, total: 5min 38s\n",
      "Wall time: 6min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('\\nIniciando o Ajuste Fino do LLM... Seja Paciente e Aguarde!')\n",
    "\n",
    "# Iniciando o loop para as épocas de treinamento\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Iterando por cada batch (conjunto) de dados de entrada e alvos no dataset de treinamento\n",
    "    for source, targets in dados:\n",
    "\n",
    "        # Resetando os gradientes acumulados no otimizador\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculando a perda (loss) através da entropia cruzada \n",
    "        # entre as previsões do modelo e os alvos verdadeiros. \n",
    "        # Os tensores são \"achatados\" para que possam ser passados para a função de entropia cruzada. \n",
    "        # O índice do token de preenchimento (pad_token) é ignorado no cálculo da perda.\n",
    "        loss = F.cross_entropy(modelo(source).logits.flatten(end_dim = 1), \n",
    "                               targets.flatten(end_dim = 1), \n",
    "                               ignore_index = tokenizer.pad_token_id)\n",
    "\n",
    "        # Calculando os gradientes da perda em relação aos parâmetros do modelo\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        # Atualizando os parâmetros do modelo utilizando os gradientes calculados\n",
    "        optimizer.step()\n",
    "\n",
    "        # Recalculando a perda após a etapa de otimização. \n",
    "        loss = F.cross_entropy(modelo(source).logits.flatten(end_dim = 1), \n",
    "                               targets.flatten(end_dim = 1), \n",
    "                               ignore_index = tokenizer.pad_token_id)\n",
    "\n",
    "    # Imprimindo a época atual e a perda após cada época de treinamento\n",
    "    print(f'\\nEpoch: {epoch+1} / {num_epochs} --- Erro: {loss.item()}')\n",
    "\n",
    "print('\\nAjuste Fino do LLM Concluído com Sucesso.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a função gera_solution com três parâmetros: input, solution_length e model\n",
    "def dsa_faz_previsao(entrada, solution_length = 6, model = modelo):\n",
    "\n",
    "    # Colocando o modelo em modo de avaliação. \n",
    "    model.eval()\n",
    "\n",
    "    # Convertendo a entrada (string) em tensor utilizando o tokenizer. \n",
    "    # O tensor é uma estrutura de dados que o modelo de aprendizado de máquina pode processar.\n",
    "    entrada = torch.tensor(tokenizer(entrada))\n",
    "\n",
    "    # Enviando o tensor de entrada para o dispositivo de cálculo disponível (CPU ou GPU)\n",
    "    entrada = entrada.to(accelerator.device)\n",
    "\n",
    "    # Iniciando uma lista vazia para armazenar a solução\n",
    "    solution = []\n",
    "\n",
    "    # Loop que gera a solução de comprimento solution_length\n",
    "    for i in range(solution_length):\n",
    "\n",
    "        # Alimentando a entrada atual ao modelo e obtendo a saída\n",
    "        saida = model(entrada)\n",
    "\n",
    "        # Pegando o índice do maior valor no último conjunto de logits (log-odds) da saída, \n",
    "        # que é a previsão do modelo para o próximo token\n",
    "        predicted = saida.logits[-1].argmax()\n",
    "\n",
    "        # Concatenando a previsão atual com a entrada atual. \n",
    "        # Isso servirá como a nova entrada para a próxima iteração.\n",
    "        entrada = torch.cat((entrada, predicted.unsqueeze(0)), dim = 0)\n",
    "\n",
    "        # Adicionando a previsão atual à lista de soluções e convertendo o tensor em um número Python padrão\n",
    "        solution.append(predicted.cpu().item())\n",
    "\n",
    "    # Decodificando a lista de soluções para obter a string de saída e retornando-a\n",
    "    return tokenizer.decode(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a função avalia_modelo com dois parâmetros: num_samples e log\n",
    "def dsa_avalia_modelo(num_samples = 1000, log = False):\n",
    "\n",
    "    # Iniciando um contador para as previsões corretas\n",
    "    correct = 0\n",
    "\n",
    "    # Loop que itera num_samples vezes\n",
    "    for i in range(num_samples):\n",
    "\n",
    "        # Obtendo a entrada e o alvo (resposta correta) do i-ésimo exemplo do conjunto de teste\n",
    "        entrada, target = dataset_teste[i]\n",
    "\n",
    "        # Convertendo os tensores de entrada e alvo em arrays numpy para processamento posterior\n",
    "        entrada = entrada.cpu().numpy()\n",
    "        target = target.cpu().numpy()\n",
    "\n",
    "        # Decodificando a entrada e o alvo utilizando o tokenizer\n",
    "        entrada = tokenizer.decode(entrada[:sequence_length])\n",
    "        target = tokenizer.decode(target[sequence_length-1:])\n",
    "\n",
    "        # Gerando a previsão utilizando a função faz_previsao\n",
    "        predicted = dsa_faz_previsao(entrada, solution_length = result_length, model = modelo)\n",
    " \n",
    "        # Se a previsão for igual ao alvo, incrementa o contador de previsões corretas\n",
    "        if target == predicted:\n",
    "            correct += 1\n",
    "            # Se log for True, imprime detalhes do exemplo e a previsão correta\n",
    "            if log:\n",
    "                print(f'Acerto do Modelo: Input: {entrada} Target: {target} Previsão: {predicted}')\n",
    "        else:\n",
    "            # Se log for True, imprime detalhes do exemplo e a previsão errada\n",
    "            if log:\n",
    "                print(f'Erro do Modelo: Input: {entrada} Target: {target} Previsão: {predicted}')\n",
    "\n",
    "    # Ao final do loop, calcula a acurácia (número de previsões corretas dividido pelo número total de exemplos) \n",
    "    print(f'\\nAcurácia: {correct/num_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acerto do Modelo: Input: 7 + 2 = Target: 0 9 Previsão: 0 9\n",
      "Acerto do Modelo: Input: 4 + 6 = Target: 1 0 Previsão: 1 0\n",
      "Acerto do Modelo: Input: 5 + 1 = Target: 0 6 Previsão: 0 6\n",
      "Acerto do Modelo: Input: 0 + 5 = Target: 0 5 Previsão: 0 5\n",
      "Acerto do Modelo: Input: 8 + 7 = Target: 1 5 Previsão: 1 5\n",
      "Acerto do Modelo: Input: 6 + 4 = Target: 1 0 Previsão: 1 0\n",
      "Acerto do Modelo: Input: 0 + 2 = Target: 0 2 Previsão: 0 2\n",
      "Acerto do Modelo: Input: 1 + 5 = Target: 0 6 Previsão: 0 6\n",
      "Acerto do Modelo: Input: 9 + 7 = Target: 1 6 Previsão: 1 6\n",
      "Acerto do Modelo: Input: 5 + 1 = Target: 0 6 Previsão: 0 6\n",
      "\n",
      "Acurácia: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Executa a função\n",
    "dsa_avalia_modelo(num_samples = 10, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acurácia: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Executa a função\n",
    "dsa_avalia_modelo(num_samples = 1000, log = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.save_pretrained(\"llm/llm_dsa_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%watermark -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
